{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahmed Hisham Elsayed\n",
    "\n",
    "Email: A.HishamElsayed@gmail.com\n",
    "\n",
    "linkedIn: www.linkedin.com/in/etch-7387\n",
    "\n",
    "Websit: https://datawins.wuiltsite.com/\n",
    "\n",
    "Tel: +201096979894"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Getting to Philosophy\n",
    " \n",
    "Please write a Python script to check the \"Getting to Philosophy\" law.\n",
    "https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy\n",
    " \n",
    "Clicking on the first link in the main body of a Wikipedia article and repeating the process for subsequent articles would usually lead to the article Philosophy.\n",
    " \n",
    "The program should receive a Wikipedia link as an input, go to another normal link and repeat this process until either Philosophy page is reached, or we are in an article without any outgoing Wikilinks, or stuck in a loop.\n",
    " \n",
    "A \"normal link\" is a link from the main page article, not in a box, is blue (red is for non-existing articles), not in parentheses, not italic and not a footnote. You don't have to check style tables or other fancy things, it is enough that the script works with the current Wikipedia style (for example you can use 'class' attribute in Wikipedia tags). For easy validation, please print all visited links to the standard output.\n",
    " \n",
    "Use a 0.5 second timeout between queries to avoid heavy load on Wikipedia (sleep function from time module).\n",
    " \n",
    "You can use https://en.wikipedia.org/wiki/Special:Random to check this hypothesis at home.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages needed \n",
    "import time\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kick off\n",
    "kickOff_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "# target\n",
    "target_url = \"https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy\"\n",
    "# store the visited article in a list\n",
    "visited_urls = [kickOff_url]\n",
    "\n",
    "def find_first_link(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # This variable stars with the body of the article\n",
    "    content = soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
    "\n",
    "    # if the link contains no links it remains None\n",
    "    article_link = None\n",
    "\n",
    "    # Find all the subpages of content that are paragraphs\n",
    "    for element in content.find_all(\"p\", recursive=False):\n",
    "        #find only the direct subpages\n",
    "        if element.find(\"a\", recursive=False):\n",
    "            article_link = element.find(\"a\", recursive=False).get('href')\n",
    "            break\n",
    "\n",
    "    if not article_link:\n",
    "        return\n",
    "\n",
    "    # Build a full url \n",
    "    first_link = urllib.parse.urljoin(\n",
    "        'https://en.wikipedia.org/', article_link)\n",
    "\n",
    "    return first_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "https://en.wikipedia.org/wiki/Soccer\n",
      "https://en.wikipedia.org/wiki/Team_sport\n",
      "https://en.wikipedia.org/wiki/Sport\n",
      "https://en.wikipedia.org/wiki/Competition\n",
      "https://en.wikipedia.org/wiki/Goal\n",
      "https://en.wikipedia.org/wiki/Idea\n",
      "https://en.wikipedia.org/wiki/Philosophy\n",
      "https://en.wikipedia.org/wiki/Greek_language\n",
      "Arrived at an article with no links, search aborted.\n"
     ]
    }
   ],
   "source": [
    "def scraping(scraping_history, target_url, max_steps=100):\n",
    "    # When reaches to philosphy\n",
    "    if scraping_history[-1] == target_url:\n",
    "        print(\"We Reached'Philosphy'article\")\n",
    "        return False\n",
    "    # max iterations \n",
    "    elif len(scraping_history) > max_steps:\n",
    "        print(\"Maximum '50' searches reached, Sorry we had to stop.\")\n",
    "        return False\n",
    "    elif scraping_history[-1] in scraping_history[:-1]:\n",
    "        print(\"Shit!-> Loop , Sorry we had to stop.\")\n",
    "        return False\n",
    "    else:\n",
    "        return True;\n",
    "    \n",
    "\n",
    "while scraping(visited_urls, target_url):\n",
    "    #print first link\n",
    "    print(visited_urls[-1])\n",
    "\n",
    "    first_link = find_first_link(visited_urls[-1])\n",
    "    # when arrive at an article with no links\n",
    "    if not first_link:\n",
    "        print(\"Arrived at an article with no links, search aborted.\")\n",
    "        break\n",
    "\n",
    "    visited_urls.append(first_link)\n",
    "\n",
    "    time.sleep(0.5)  # Slow things down so as to not overload Wikipedia's servers\n",
    "visited_urls=[start_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
